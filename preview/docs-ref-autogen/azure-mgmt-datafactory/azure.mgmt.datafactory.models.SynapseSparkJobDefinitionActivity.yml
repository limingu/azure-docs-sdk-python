### YamlMime:PythonClass
uid: azure.mgmt.datafactory.models.SynapseSparkJobDefinitionActivity
name: SynapseSparkJobDefinitionActivity
fullName: azure.mgmt.datafactory.models.SynapseSparkJobDefinitionActivity
module: azure.mgmt.datafactory.models
inheritances:
- azure.mgmt.datafactory.models._models_py3.ExecutionActivity
summary: 'Execute spark job activity.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'SynapseSparkJobDefinitionActivity(*, name: str, spark_job: _models.SynapseSparkJobReference,
    additional_properties: Optional[Dict[str, collections.abc.MutableMapping[str,
    Any]]] = None, description: Optional[str] = None, depends_on: Optional[List[_models.ActivityDependency]]
    = None, user_properties: Optional[List[_models.UserProperty]] = None, linked_service_name:
    Optional[_models.LinkedServiceReference] = None, policy: Optional[_models.ActivityPolicy]
    = None, arguments: Optional[List[Any]] = None, file: Optional[collections.abc.MutableMapping[str,
    Any]] = None, scan_folder: Optional[collections.abc.MutableMapping[str, Any]]
    = None, class_name: Optional[collections.abc.MutableMapping[str, Any]] = None,
    files: Optional[List[collections.abc.MutableMapping[str, Any]]] = None, python_code_reference:
    Optional[List[collections.abc.MutableMapping[str, Any]]] = None, files_v2: Optional[List[collections.abc.MutableMapping[str,
    Any]]] = None, target_big_data_pool: Optional[_models.BigDataPoolParametrizationReference]
    = None, executor_size: Optional[collections.abc.MutableMapping[str, Any]] = None,
    conf: Optional[collections.abc.MutableMapping[str, Any]] = None, driver_size:
    Optional[collections.abc.MutableMapping[str, Any]] = None, num_executors: Optional[collections.abc.MutableMapping[str,
    Any]] = None, configuration_type: Optional[Union[str, _models.ConfigurationType]]
    = None, target_spark_configuration: Optional[_models.SparkConfigurationParametrizationReference]
    = None, spark_config: Optional[Dict[str, collections.abc.MutableMapping[str, Any]]]
    = None, **kwargs: Any)'
variables:
- description: 'Unmatched properties from the message are deserialized to this

    collection.'
  name: additional_properties
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: Activity name. Required.
  name: name
  types:
  - <xref:str>
- description: Type of activity. Required.
  name: type
  types:
  - <xref:str>
- description: Activity description.
  name: description
  types:
  - <xref:str>
- description: Activity depends on condition.
  name: depends_on
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.ActivityDependency>]
- description: Activity user properties.
  name: user_properties
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.UserProperty>]
- description: Linked service reference.
  name: linked_service_name
  types:
  - <xref:azure.mgmt.datafactory.models.LinkedServiceReference>
- description: Activity policy.
  name: policy
  types:
  - <xref:azure.mgmt.datafactory.models.ActivityPolicy>
- description: Synapse spark job reference. Required.
  name: spark_job
  types:
  - <xref:azure.mgmt.datafactory.models.SynapseSparkJobReference>
- description: User specified arguments to SynapseSparkJobDefinitionActivity.
  name: arguments
  types:
  - <xref:list>[<xref:any>]
- description: 'The main file used for the job, which will override the ''file'' of
    the spark job

    definition you provide. Type: string (or Expression with resultType string).'
  name: file
  types:
  - <xref:JSON>
- description: 'Scanning subfolders from the root folder of the main definition file,
    these

    files will be added as reference files. The folders named ''jars'', ''pyFiles'',
    ''files'' or

    ''archives'' will be scanned, and the folders name are case sensitive. Type: boolean
    (or

    Expression with resultType boolean).'
  name: scan_folder
  types:
  - <xref:JSON>
- description: 'The fully-qualified identifier or the main class that is in the main

    definition file, which will override the ''className'' of the spark job definition
    you provide.

    Type: string (or Expression with resultType string).'
  name: class_name
  types:
  - <xref:JSON>
- description: '(Deprecated. Please use pythonCodeReference and filesV2) Additional
    files used for

    reference in the main definition file, which will override the ''files'' of the
    spark job

    definition you provide.'
  name: files
  types:
  - <xref:list>[<xref:JSON>]
- description: 'Additional python code files used for reference in the main

    definition file, which will override the ''pyFiles'' of the spark job definition
    you provide.'
  name: python_code_reference
  types:
  - <xref:list>[<xref:JSON>]
- description: 'Additional files used for reference in the main definition file, which
    will

    override the ''jars'' and ''files'' of the spark job definition you provide.'
  name: files_v2
  types:
  - <xref:list>[<xref:JSON>]
- description: 'The name of the big data pool which will be used to execute the

    spark batch job, which will override the ''targetBigDataPool'' of the spark job
    definition you

    provide.'
  name: target_big_data_pool
  types:
  - <xref:azure.mgmt.datafactory.models.BigDataPoolParametrizationReference>
- description: 'Number of core and memory to be used for executors allocated in the

    specified Spark pool for the job, which will be used for overriding ''executorCores''
    and

    ''executorMemory'' of the spark job definition you provide. Type: string (or Expression
    with

    resultType string).'
  name: executor_size
  types:
  - <xref:JSON>
- description: 'Spark configuration properties, which will override the ''conf'' of
    the spark job

    definition you provide.'
  name: conf
  types:
  - <xref:JSON>
- description: 'Number of core and memory to be used for driver allocated in the specified

    Spark pool for the job, which will be used for overriding ''driverCores'' and
    ''driverMemory'' of

    the spark job definition you provide. Type: string (or Expression with resultType
    string).'
  name: driver_size
  types:
  - <xref:JSON>
- description: 'Number of executors to launch for this job, which will override the

    ''numExecutors'' of the spark job definition you provide. Type: integer (or Expression
    with

    resultType integer).'
  name: num_executors
  types:
  - <xref:JSON>
- description: 'The type of the spark config. Known values are: "Default",

    "Customized", and "Artifact".'
  name: configuration_type
  types:
  - <xref:str>
  - <xref:azure.mgmt.datafactory.models.ConfigurationType>
- description: The spark configuration of the spark job.
  name: target_spark_configuration
  types:
  - <xref:azure.mgmt.datafactory.models.SparkConfigurationParametrizationReference>
- description: Spark configuration property.
  name: spark_config
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
