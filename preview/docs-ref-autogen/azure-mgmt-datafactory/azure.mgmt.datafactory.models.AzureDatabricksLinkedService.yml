### YamlMime:PythonClass
uid: azure.mgmt.datafactory.models.AzureDatabricksLinkedService
name: AzureDatabricksLinkedService
fullName: azure.mgmt.datafactory.models.AzureDatabricksLinkedService
module: azure.mgmt.datafactory.models
inheritances:
- azure.mgmt.datafactory.models._models_py3.LinkedService
summary: 'Azure Databricks linked service.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'AzureDatabricksLinkedService(*, domain: collections.abc.MutableMapping[str,
    Any], additional_properties: Optional[Dict[str, collections.abc.MutableMapping[str,
    Any]]] = None, connect_via: Optional[_models.IntegrationRuntimeReference] = None,
    description: Optional[str] = None, parameters: Optional[Dict[str, _models.ParameterSpecification]]
    = None, annotations: Optional[List[collections.abc.MutableMapping[str, Any]]]
    = None, access_token: Optional[_models.SecretBase] = None, authentication: Optional[collections.abc.MutableMapping[str,
    Any]] = None, workspace_resource_id: Optional[collections.abc.MutableMapping[str,
    Any]] = None, existing_cluster_id: Optional[collections.abc.MutableMapping[str,
    Any]] = None, instance_pool_id: Optional[collections.abc.MutableMapping[str, Any]]
    = None, new_cluster_version: Optional[collections.abc.MutableMapping[str, Any]]
    = None, new_cluster_num_of_worker: Optional[collections.abc.MutableMapping[str,
    Any]] = None, new_cluster_node_type: Optional[collections.abc.MutableMapping[str,
    Any]] = None, new_cluster_spark_conf: Optional[Dict[str, collections.abc.MutableMapping[str,
    Any]]] = None, new_cluster_spark_env_vars: Optional[Dict[str, collections.abc.MutableMapping[str,
    Any]]] = None, new_cluster_custom_tags: Optional[Dict[str, collections.abc.MutableMapping[str,
    Any]]] = None, new_cluster_log_destination: Optional[collections.abc.MutableMapping[str,
    Any]] = None, new_cluster_driver_node_type: Optional[collections.abc.MutableMapping[str,
    Any]] = None, new_cluster_init_scripts: Optional[collections.abc.MutableMapping[str,
    Any]] = None, new_cluster_enable_elastic_disk: Optional[collections.abc.MutableMapping[str,
    Any]] = None, encrypted_credential: Optional[collections.abc.MutableMapping[str,
    Any]] = None, policy_id: Optional[collections.abc.MutableMapping[str, Any]] =
    None, credential: Optional[_models.CredentialReference] = None, **kwargs: Any)'
variables:
- description: 'Unmatched properties from the message are deserialized to this

    collection.'
  name: additional_properties
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: Type of linked service. Required.
  name: type
  types:
  - <xref:str>
- description: The integration runtime reference.
  name: connect_via
  types:
  - <xref:azure.mgmt.datafactory.models.IntegrationRuntimeReference>
- description: Linked service description.
  name: description
  types:
  - <xref:str>
- description: Parameters for linked service.
  name: parameters
  types:
  - <xref:dict>[<xref:str>, <xref:azure.mgmt.datafactory.models.ParameterSpecification>]
- description: List of tags that can be used for describing the linked service.
  name: annotations
  types:
  - <xref:list>[<xref:JSON>]
- description: '`<REGION>`.azuredatabricks.net, domain name of your Databricks deployment.

    Type: string (or Expression with resultType string). Required.'
  name: domain
  types:
  - <xref:JSON>
- description: 'Access token for databricks REST API. Refer to

    [https://docs.azuredatabricks.net/api/latest/authentication.html](https://docs.azuredatabricks.net/api/latest/authentication.html).
    Type: string (or Expression

    with resultType string).'
  name: access_token
  types:
  - <xref:azure.mgmt.datafactory.models.SecretBase>
- description: 'Required to specify MSI, if using Workspace resource id for databricks

    REST API. Type: string (or Expression with resultType string).'
  name: authentication
  types:
  - <xref:JSON>
- description: 'Workspace resource id for databricks REST API. Type: string (or

    Expression with resultType string).'
  name: workspace_resource_id
  types:
  - <xref:JSON>
- description: 'The id of an existing interactive cluster that will be used for all

    runs of this activity. Type: string (or Expression with resultType string).'
  name: existing_cluster_id
  types:
  - <xref:JSON>
- description: 'The id of an existing instance pool that will be used for all runs
    of

    this activity. Type: string (or Expression with resultType string).'
  name: instance_pool_id
  types:
  - <xref:JSON>
- description: 'If not using an existing interactive cluster, this specifies the

    Spark version of a new job cluster or instance pool nodes created for each run
    of this

    activity. Required if instancePoolId is specified. Type: string (or Expression
    with resultType

    string).'
  name: new_cluster_version
  types:
  - <xref:JSON>
- description: 'If not using an existing interactive cluster, this specifies

    the number of worker nodes to use for the new job cluster or instance pool. For
    new job

    clusters, this a string-formatted Int32, like ''1'' means numOfWorker is 1 or
    ''1:10'' means

    auto-scale from 1 (min) to 10 (max). For instance pools, this is a string-formatted
    Int32, and

    can only specify a fixed number of worker nodes, such as ''2''. Required if newClusterVersion
    is

    specified. Type: string (or Expression with resultType string).'
  name: new_cluster_num_of_worker
  types:
  - <xref:JSON>
- description: 'The node type of the new job cluster. This property is required if

    newClusterVersion is specified and instancePoolId is not specified. If instancePoolId
    is

    specified, this property is ignored. Type: string (or Expression with resultType
    string).'
  name: new_cluster_node_type
  types:
  - <xref:JSON>
- description: 'A set of optional, user-specified Spark configuration key-value

    pairs.'
  name: new_cluster_spark_conf
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'A set of optional, user-specified Spark environment variables

    key-value pairs.'
  name: new_cluster_spark_env_vars
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'Additional tags for cluster resources. This property is ignored

    in instance pool configurations.'
  name: new_cluster_custom_tags
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'Specify a location to deliver Spark driver, worker, and

    event logs. Type: string (or Expression with resultType string).'
  name: new_cluster_log_destination
  types:
  - <xref:JSON>
- description: 'The driver node type for the new job cluster. This property

    is ignored in instance pool configurations. Type: string (or Expression with resultType

    string).'
  name: new_cluster_driver_node_type
  types:
  - <xref:JSON>
- description: 'User-defined initialization scripts for the new cluster. Type:

    array of strings (or Expression with resultType array of strings).'
  name: new_cluster_init_scripts
  types:
  - <xref:JSON>
- description: 'Enable the elastic disk on the new cluster. This

    property is now ignored, and takes the default elastic disk behavior in Databricks
    (elastic

    disks are always enabled). Type: boolean (or Expression with resultType boolean).'
  name: new_cluster_enable_elastic_disk
  types:
  - <xref:JSON>
- description: 'The encrypted credential used for authentication. Credentials are

    encrypted using the integration runtime credential manager. Type: string (or Expression
    with

    resultType string).'
  name: encrypted_credential
  types:
  - <xref:JSON>
- description: 'The policy id for limiting the ability to configure clusters based
    on a user

    defined set of rules. Type: string (or Expression with resultType string).'
  name: policy_id
  types:
  - <xref:JSON>
- description: The credential reference containing authentication information.
  name: credential
  types:
  - <xref:azure.mgmt.datafactory.models.CredentialReference>
